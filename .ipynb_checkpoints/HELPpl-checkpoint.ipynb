{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c24c5d",
   "metadata": {},
   "source": [
    "Import- Hyperlink to documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "ac57a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import keras.regularizers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE, SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6c7f5-ec16-41d9-a34a-17588eeca4a9",
   "metadata": {},
   "source": [
    "# Supervised Hybrid Production-Level Long Short Term Memory Recurrent Neural Network:\n",
    "#### _Semi-Automating proccesses for building and optimization_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa54c2-ecaa-446a-a7ae-4a91ca280919",
   "metadata": {},
   "source": [
    "## Forecasting Absolute Humidity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c6f98-fdbd-415a-b50f-dd5f7a1b0d5e",
   "metadata": {},
   "source": [
    "### Question: Can we predict humidity in San Diego for the next day based on a given humidity, wind speed, wind direction, and pressure for that hour?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9d44b-86db-4ea2-9123-8224da5386c3",
   "metadata": {},
   "source": [
    "An LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) that is designed to handle long-term dependencies in sequential data. It uses a memory cell that can store information for long periods of time and a set of gates to control the flow of information into and out of the cell. The gates are used to selectively forget or remember information based on the input and the current state of the cell. LSTMs have been shown to be effective in a wide range of applications including natural language processing, speech recognition, and time series prediction.\n",
    "The chosen model is a recurrent neural network (RNN) with long short-term memory (LSTM) cells. The reason for this choice is that LSTMs are specifically designed for handling time-series data, which makes them well-suited for forecasting tasks. \n",
    "LSTMs have the ability to learn long-term dependencies in the data, which is important for capturing the patterns and trends that may exist in the time-series data. Additionally, LSTMs are capable of handling sequential data of varying lengths, which is useful for this problem since the number of time steps in the input data may vary. LSTMs are often preferred for time-series forecasting problems due to their ability to capture long-term dependencies and their flexibility in handling sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c82bae-1bd5-4769-8250-924619d7fa20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "643a90a3-fe94-4b0a-8f5f-13bcfabb03b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "4b36c696-9fc0-4cbc-8712-a6078b4d423c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "humidity= pd.read_csv('clean_data/cleaned_humidity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "8c817d97-8b85-4963-a0bf-da6549ffeca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pressure= pd.read_csv('clean_data/cleaned_pressure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "81972f15-baba-4fb3-80fc-6900c35c6c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wind_speed= pd.read_csv('clean_data/cleaned_ws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "9b0fbbd1-f645-47ba-9961-1324b74fe76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wind_dir= pd.read_csv('clean_data/cleaned_wr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "5b2a0325-b1e2-48df-8c91-a7d8f0c714dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename the columns in the original DataFrames\n",
    "humidity.columns = [f'humidity_{col}' for col in humidity.columns]\n",
    "pressure.columns = [f'pressure_{col}' for col in pressure.columns]\n",
    "wind_dir.columns = [f'wind_dir_{col}' for col in wind_dir.columns]\n",
    "wind_speed.columns = [f'wind_speed_{col}' for col in wind_speed.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "e031bad3-1ff3-4bdf-ad7c-b2206e372f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames along the columns axis\n",
    "san_diego_df = pd.concat([humidity[['humidity_datetime']], humidity.drop('humidity_datetime', axis=1), pressure, wind_dir, wind_speed], axis=1)\n",
    "\n",
    "san_diego_columns = ['humidity_datetime'] + [col for col in san_diego_df.columns if 'San Diego' in col]\n",
    "san_diego_df = san_diego_df[san_diego_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "c07da7d7-6eaf-4e79-bfcd-84e5e8e16d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rename the columns\n",
    "san_diego_df= san_diego_df.rename(columns={\"humidity_datetime\": \"datetime\", \"humidity_San Diego\": \"humidity\", \"pressure_San Diego\":\"pressure\", \"wind_dir_San Diego\":\"wind_dir\", \"wind_speed_San Diego\": \"wind_speed\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "9b7fc8bb-e3ef-4f29-9e1a-7d2e1526e6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Extract the hour from the datetime column\n",
    "san_diego_df['hour'] = pd.to_datetime(san_diego_df['datetime']).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "8d743d89-c029-4749-8a8a-75e98b210316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Drop datetime\n",
    "san_diego_df.drop(['datetime'], inplace= True, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "99aed5f2-ad81-4a7e-846e-7cee0978a2e0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humidity distribution:\n",
      "normal: D=0.07225708153364296, p-value=9.521463621270879e-201\n",
      "exponential: D=0.3549962526547573, p-value=0.0\n",
      "logistic: D=0.0565723130142094, p-value=3.622505804104062e-123\n",
      "\n",
      "pressure distribution:\n",
      "normal: D=0.14996863978091823, p-value=0.0\n",
      "exponential: D=0.5242211365851986, p-value=0.0\n",
      "logistic: D=0.09150311488892317, p-value=0.0\n",
      "\n",
      "wind_speed distribution:\n",
      "normal: D=0.26540812964324156, p-value=0.0\n",
      "exponential: D=0.2836830540579809, p-value=0.0\n",
      "logistic: D=0.24111324355792074, p-value=0.0\n",
      "\n",
      "wind_dir distribution:\n",
      "normal: D=0.13003959649634, p-value=0.0\n",
      "exponential: D=0.22719236582053187, p-value=0.0\n",
      "logistic: D=0.10435693010217906, p-value=0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating a function to use statistcal tests to determine distribution\n",
    "weather_elements = ['humidity', 'pressure', 'wind_speed', 'wind_dir']\n",
    "\n",
    "san_diego_df = san_diego_df.applymap(lambda x: x if np.isfinite(x) else np.nan)\n",
    "\n",
    "def test_distributions(series):\n",
    "    \n",
    "    \n",
    "    series = series[np.isfinite(series)]\n",
    "    \n",
    "    distributions = {\n",
    "        \"normal\": stats.norm,\n",
    "        \"exponential\": stats.expon,\n",
    "        \"logistic\": stats.logistic\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for dist_name, dist in distributions.items():\n",
    "        params = dist.fit(series)\n",
    "        D, p = stats.kstest(series, dist.cdf, args=params)\n",
    "        results[dist_name] = (D, p)\n",
    "    \n",
    "    return results\n",
    "\n",
    "for element in weather_elements:\n",
    "    print(f\"{element} distribution:\")\n",
    "    results = test_distributions(san_diego_df[element])\n",
    "    for dist_name, (D, p) in results.items():\n",
    "        print(f\"{dist_name}: D={D}, p-value={p}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ecfd9d92-58c3-4441-a614-f12a4102ef92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Asid from obviously hour, there doesn't seem to be any pattern of distribution that I can identify at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab2afd-1712-4fb0-b506-ddd56b0fcaaf",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering and Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5366144a-d46d-4577-81af-33238066cc64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_diego_df.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "bfa228f5-db60-4ceb-83fb-ac1abb5aac23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dropping Nulls\n",
    "san_diego_df = san_diego_df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "fe56f636-1f68-4d46-9645-34246c5d5345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44107, 5)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_diego_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "455fde88-cd3b-4b0a-a1ff-7609e35921ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Making a column for my target\n",
    "san_diego_df['next_day_humidity'] = san_diego_df['humidity'].shift(-24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "816d25ba-e56e-4d34-aa5b-cb544b18c529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['humidity', 'pressure', 'wind_dir', 'wind_speed', 'hour',\n",
       "       'next_day_humidity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_diego_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "8b5ef9ce-11f9-4192-820f-04de89fbe029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Identifying and quantifying outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "a48c977b-52cd-4111-a561-ca3d87c18f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df, threshold=3):\n",
    "    outliers = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "        outliers_col = df[z_scores > threshold][col]\n",
    "        outliers_col.name = col\n",
    "        outliers = pd.concat([outliers, outliers_col], axis=1)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "8a998788-3b9f-499a-ae24-86d8dad6868f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_outlier_list(df, threshold):\n",
    "    outlier_list = {}\n",
    "    outlier = detect_outliers_zscore(df, threshold)\n",
    "    for col in outlier.columns:\n",
    "        non_null_mask = outlier[col].notnull()\n",
    "        if non_null_mask.any():\n",
    "            outlier_list[col] = outlier.loc[non_null_mask, col].values.tolist()\n",
    "    \n",
    "    return outlier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "bb2cf526-f807-4cf1-b841-d3e0006ccc01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers= get_outlier_list(san_diego_df, threshold= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "73e415a1-9251-4be0-af14-e30743356723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humidity: 113\n",
      "pressure: 1127\n",
      "wind_speed: 363\n",
      "next_day_humidity: 113\n"
     ]
    }
   ],
   "source": [
    "for element, values in outliers.items():\n",
    "    print(f\"{element}: {len(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "13f9dc23-b6de-4618-83e6-83277635f079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#A small amount of outliers for the given data set- moderation is not necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488182c-c24e-4d87-98e5-2987465bf7bf",
   "metadata": {},
   "source": [
    "Applying the Winsorize method to moderate outliers as a display of proficiency in selecting and applying statistical models and best data practices\n",
    "Reasons for Selection: \n",
    "1. No normal distribution \n",
    "2. To maintain data integrity by only removing the extreme outliers- Extreme outliers might indicate faulty data\n",
    "\n",
    "https://www.statisticshowto.com/winsorize/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "bf6bcc71-64db-444e-89d8-81624dc58909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Defining percentile to drop\n",
    "percentiles = [0.01, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "c552dc45-1fe1-492e-ac9e-43cba4a9ff1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Iterating the method over each feature\n",
    "for col in weather_elements:\n",
    "    san_diego_df[col] = winsorize(san_diego_df[col], limits=percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "604e0d4e-e0bf-4248-88c0-51c82e5a0aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Encoding catergorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "a265db33-3304-40a2-8309-d13a3803d70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circular encoding for wind direction\n",
    "san_diego_df['wind_dir_sin'] = np.sin(2*np.pi*san_diego_df['wind_dir']/360)\n",
    "san_diego_df['wind_dir_cos'] = np.cos(2*np.pi*san_diego_df['wind_dir']/360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "a28de36c-eefd-4eb2-b60f-d7f35737f5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the original wind_direction column\n",
    "san_diego_df.drop('wind_dir', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "64114f26-021b-49b6-b3c6-46bd162c433a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#One Hot Encoding 'hour'\n",
    "#Get the dummies for hour\n",
    "one_hot = pd.get_dummies(san_diego_df[\"hour\"], prefix=\"hour\")\n",
    "#Concat the dummy column\n",
    "san_diego_df = pd.concat([san_diego_df, one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "ad9a5a00-e3e6-4e07-9215-14d7d43bb56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dropping original hour\n",
    "san_diego_df.drop(\"hour\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "e27528a7-8963-4175-95ef-5c17a025847c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Engineering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "d6167e77-45fd-4419-8ea8-7b1bcdbe8124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Capturing the temporal relationship between the current and previous hour's humidity values\n",
    "san_diego_df['prev_hour_humidity'] = san_diego_df['humidity'].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914e4f6-001a-4473-a4aa-80d380889099",
   "metadata": {},
   "source": [
    "seasonal_decompose() is a function from the statsmodels library that decomposes a time series into its trend, seasonality, and residual components. The resulting components can help identify patterns and relationships in the time series data that may not be easily observable in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "d1d8a9c1-c466-4ef9-8617-a3067b689ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = seasonal_decompose(san_diego_df['humidity'], model='additive', period=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "dac0c490-d5f8-436f-bfc2-d72e204bca8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#long-term trend in the humidity data\n",
    "san_diego_df['humidity_trend'] = result.trend\n",
    "#recurring seasonal patterns\n",
    "san_diego_df['humidity_seasonality'] = result.seasonal\n",
    "# unexplained variance or randomness\n",
    "san_diego_df['humidity_residual'] = result.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "db9a4f6b-54bf-4752-9ce2-f30b79080fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['humidity', 'pressure', 'wind_speed', 'next_day_humidity',\n",
       "       'wind_dir_sin', 'wind_dir_cos', 'hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
       "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
       "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
       "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
       "       'hour_23', 'prev_hour_humidity', 'humidity_trend',\n",
       "       'humidity_seasonality', 'humidity_residual'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_diego_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "bab6a8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44107, 34)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_diego_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "8eaa95a6-58f9-426b-a1f1-2515e2dce550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Seperating Features and Target into X and y\n",
    "X = san_diego_df.drop('next_day_humidity', axis=1)\n",
    "y = san_diego_df[['next_day_humidity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "3a915ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def preprocess_data(X):\n",
    "    # Impute missing values in X\n",
    "    imputer_X = SimpleImputer()\n",
    "    X = imputer_X.fit_transform(X)\n",
    "        \n",
    "    # Replace infinite values with NaN in X\n",
    "    X = np.where(np.isfinite(X), X, np.nan)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "1b5b1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= preprocess_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "d701450a-abcc-4ef9-a41d-fe00f150e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Asid from obviously hour, there doesn't seem to be any pattern of distribution that I can identify at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf904906-1acc-4602-b63d-b733f4a1b14c",
   "metadata": {},
   "source": [
    "## 3. Model Building:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e21b66",
   "metadata": {},
   "source": [
    "### Build\n",
    "\n",
    "_This code defines a build function that trains an LSTM (Long Short-Term Memory) model to predict the target variable y based on input features X. It explores different feature selection functions, cross-validation methods, and hyperparameters to find the best model. The performance metric used for model evaluation is the mean absolute error (MAE)._\n",
    "\n",
    "#### Broader Picture and Cross Applications\n",
    "The LSTM model is a type of recurrent neural network (RNN) that excels in learning patterns from time series data. This function is applicable to a variety of time series prediction tasks, such as stock price forecasting, weather prediction, and energy consumption forecasting. By identifying the best feature selection method, cross-validation method, and hyperparameters, organizations can improve the accuracy of their predictions and make better data-driven decisions.\n",
    "\n",
    "#### Machine Learning Concepts\n",
    "* **LSTM:** A type of recurrent neural network designed to handle long-term dependencies in sequences, making it suitable for time series data.\n",
    "* **Feature Selection:** The process of selecting the most important features from the input data to improve model performance and reduce overfitting.\n",
    "* **Cross-Validation:** A technique to evaluate the performance of a model by partitioning the dataset into multiple subsets and training the model on each subset, testing it on the remaining data. This helps to obtain a more reliable estimate of the model's performance.\n",
    "* **Hyperparameters:** Parameters that control the learning process of a model. They are not learned by the model during training but are set beforehand. Examples include the learning rate, batch size, and number of layers in a neural network.\n",
    "\n",
    "#### Mathematical Concepts and Statistical Analysis\n",
    "* **Mean Absolute Error (MAE):** A measure of the average difference between the true values and the predicted values. It is calculated as the sum of the absolute differences between the true and predicted values divided by the number of samples.\n",
    "    _MAE = (1/n) * Î£|y_true - y_pred|_: where y_true is the true target value, y_pred is the predicted value, and n is the number of data points\n",
    "\n",
    "* **Time Series Cross-Validation:** A cross-validation technique that respects the temporal order of the data, ensuring that the model is trained on past data and evaluated on future data. This helps prevent leakage of information from the future into the past.\n",
    "\n",
    "#### Reasoning and Decisions Made\n",
    "1. The code first initializes variables to store the best results found during the search process.\n",
    "2. It then splits the dataset into training and testing data using time series cross-validation.\n",
    "3. The input data is transformed into a supervised format suitable for LSTM models.\n",
    "4. For each combination of feature selection function, and hyperparameters, the code:\n",
    "    -Performs feature selection on the input data.\n",
    "    -Creates and trains an LSTM model with the selected features and hyperparameters.\n",
    "    -Evaluates the model using cross-validation and computes the mean absolute error.\n",
    "8. Updates the best results if the current model has a lower mean absolute error.\n",
    "9. Finally, the function returns the best results, including feature selection function, mean absolute error, cross-validation method, model, training history, and hyperparameters.\n",
    "\n",
    "#### Local Outside References\n",
    "* to_supervised: A user-defined function that converts the input data into a supervised format suitable for LSTM models.\n",
    "* feature_selection_function: A user-defined function for selecting the best features from the input data.\n",
    "* get_hyperparameters: A user-defined function for obtaining a list of possible hyperparameter combinations.\n",
    "* create_lstm_model: A user-defined function that creates and trains an LSTM model with the given input data and hyperparameters.\n",
    "* cross_validate: A user-defined function that performs cross-validation for the given model and returns the mean absolute error.\n",
    "\n",
    "#### Inputs and Outputs\n",
    "* **Inputs:**\n",
    "    1. X: The input features as a 2D array or dataframe.\n",
    "    2. y: The target variable as a 1D array or dataframe.\n",
    "    3. cv_methods: A dictionary containing cross-validation methods.\n",
    "    4. metric: The performance metric used for model evaluation.\n",
    "\n",
    "* **Outputs:**\n",
    "    1. best_feature_selection_result: The best feature selection function found.\n",
    "    2. best_mae: The lowest mean absolute error achieved.\n",
    "    3. best_cv_method: The cross-validation method used for the best model.\n",
    "    4. best_model: The best LSTM model found.\n",
    "    5. best_history: The training history of the best LSTM model.\n",
    "    6. best_hyperparams: The hyperparameters used for the best LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "5dff3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric= mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "4772c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(X, y, metric):\n",
    "    # Initialize variables for the best results\n",
    "    best_mae = float('inf')\n",
    "    best_cv_method = None\n",
    "    best_model = None\n",
    "    best_history = None\n",
    "    best_hyperparams = None\n",
    "    best_feature_selection_result = None\n",
    "        \n",
    "    # Split the dataset into training and testing data using time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X = pd.DataFrame(X, columns=[\"col\"+str(i) for i in range(X.shape[1])])\n",
    "        train_data, test_data = X.iloc[train_index], X.iloc[test_index]\n",
    "        train_target, test_target = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        #Initializing Scaler\n",
    "        scaler = MinMaxScaler() #Min Max chosen due to limited outliers\n",
    "        \n",
    "        #Transforming our Features\n",
    "        test_data= scaler.fit_transform(test_data)\n",
    "        train_data = scaler.fit_transform(train_data)\n",
    "        test_target= scaler.fit_transform(test_target)\n",
    "        train_target = scaler.fit_transform(train_target)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        train_data = pd.DataFrame(train_data, columns=[\"col\"+str(i) for i in range(train_data.shape[1])])\n",
    "        train_target = pd.DataFrame(train_target, columns=[\"target\"])\n",
    "        test_data = pd.DataFrame(train_data, columns=[\"col\"+str(i) for i in range(train_data.shape[1])])\n",
    "        test_target = pd.DataFrame(train_target, columns=[\"target\"])\n",
    "        \n",
    "        # Create supervised data for LSTM model\n",
    "        train_data_with_target = pd.concat([train_data, train_target], axis=1)\n",
    "        test_data_with_target = pd.concat([test_data, test_target], axis=1)\n",
    "        X_train, y_train = to_supervised(train_data_with_target, input_steps, output_steps)\n",
    "        X_test, y_test = to_supervised(test_data_with_target, input_steps, output_steps)\n",
    "        \n",
    "        # Perform feature selection and prepare data for model training\n",
    "        X_train, X_test, best_feature_selection_result = feature_selection_function(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "        # Get hyperparameters for LSTM model\n",
    "        hyperparameters_list, hyperparameter_options = generate_hyperparameter_list()\n",
    "    \n",
    "        # Loop through hyperparameters\n",
    "        for hyperparams in hyperparameters_list:\n",
    "                    \n",
    "                # Create and train LSTM model\n",
    "                model, history = create_lstm_model(X_train, y_train, X_test, y_test, hyperparameter_list)\n",
    "                    \n",
    "                # Perform final time series cross-validation\n",
    "                val_loss, model, history = cross_validate(X, y, hyperparams, cv_method_name, cv_method, input_steps, output_steps, metric)\n",
    "\n",
    "                # Check if the current model is the best so far\n",
    "                if val_loss < best_mae:\n",
    "                    best_mae = val_loss\n",
    "                    best_cv_method = best_cv_method\n",
    "                    best_model = model\n",
    "                    best_history = history\n",
    "                    best_hyperparams = hyperparams\n",
    "                    best_feature_selection_result = best_feature_selection_result\n",
    "    \n",
    "        print(best_feature_selection_result, best_mae, best_cv_method, best_model, best_history, best_hyperparams)\n",
    "\n",
    "    # Return the best results\n",
    "    return best_feature_selection_result, best_mae, best_cv_method, best_model, best_history, best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773a3d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_mae, best_cv_method, best_model, best_history, best_hyperparams= build(X, y, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "5809ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_steps= 24\n",
    "output_steps= 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24f853",
   "metadata": {},
   "source": [
    "### To Supervised\n",
    "\n",
    "_This code defines the to_supervised function that converts time series data into a supervised learning format. Given the input data data, the function creates input/output pairs based on input_steps and output_steps. The input_steps represent the number of time steps to consider for input, while output_steps represent the number of time steps to predict in the future.__\n",
    "\n",
    "#### Broader Picture and Cross Applications\n",
    "The function is useful for transforming time series data into a format suitable for training machine learning models like LSTM or other sequence-based models. It can be applied to any time series prediction task where the goal is to predict future values based on past values, such as sales forecasting, air quality prediction, or load demand estimation in power grids.\n",
    "\n",
    "#### Reasoning and Decisions Made\n",
    "1. The function checks if input_steps and output_steps are valid positive integers.\n",
    "2. It initializes two empty lists, X and y, for storing input and output data, respectively.\n",
    "3. The function loops through the data and, for each time step, appends input_steps number of data points as input and output_steps number of data points as output.\n",
    "4. The input/output pairs are converted to numpy arrays before returning them.\n",
    "\n",
    "#### Inputs and Outputs\n",
    "* **Inputs:**\n",
    "1. data: The time series data as a 1D or 2D array or dataframe.\n",
    "2. input_steps: The number of time steps to consider for input.\n",
    "3. output_steps: The number of time steps to predict in the future.\n",
    "* **Outputs:**\n",
    "1. X: The input data in supervised learning format as a numpy array.\n",
    "2. y: The output data (future predictions) in supervised learning format as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "e86d5011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_supervised(data, input_steps, output_steps):\n",
    "    \n",
    "    # Check that input_steps and output_steps are valid\n",
    "    if not isinstance(input_steps, int) or not isinstance(output_steps, int) or input_steps <= 0 or output_steps <= 0:\n",
    "        raise ValueError(\"Input steps and output steps must be positive integers.\")\n",
    "    \n",
    "    # Initialize empty lists for input and output data\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Loop through the data to create input/output pairs\n",
    "    for i in range(len(data)-input_steps-output_steps+1):\n",
    "        # Append input data for current time step\n",
    "        X.append(data[i:i+input_steps])\n",
    "        # Append output data for current time step\n",
    "        y.append(data[i+input_steps:i+input_steps+output_steps])\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04eba76",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "_The create_lstm_model function takes training and test data, along with a list of hyperparameters, to create, train, and evaluate an LSTM model. It finds the best hyperparameters by looping through all combinations and comparing their validation losses. The function returns the best LSTM model, training history, evaluation metrics, and the best hyperparameters._\n",
    "\n",
    "#### Broader Picture and Cross Applications\n",
    "\n",
    "This function is useful for training and selecting the best LSTM model for time series prediction tasks. It can be applied to a wide range of applications, including sales forecasting, air quality prediction, and load demand estimation in power grids. The function's structure can be adapted for other types of neural networks or machine learning models, with appropriate adjustments to the model architecture and hyperparameters.\n",
    "\n",
    "#### Reasoning and Decisions Made\n",
    "1. The function initializes variables to store the best model and hyperparameters.\n",
    "2. It loops through all combinations of hyperparameters.\n",
    "3. It creates an LSTM model with the given hyperparameters and trains it using the training data.\n",
    "4. It performs cross-validation using different methods.\n",
    "5. It updates the best model if the validation loss improves.\n",
    "6. It returns the best model, training history, and evaluation metrics.\n",
    "\n",
    "#### Local Outside References\n",
    "* to_supervised function\n",
    "* cross_validate function\n",
    "* cv_methods dictionary\n",
    "* input_steps variable\n",
    "* output_steps variable\n",
    "* metric variable\n",
    "\n",
    "#### Inputs and Outputs\n",
    "* **Inputs:**\n",
    "1. X_train: The training input data in supervised learning format.\n",
    "2. y_train: The training output data in supervised learning format.\n",
    "3. X_test: The test input data in supervised learning format.\n",
    "4. y_test: The test output data in supervised learning format.\n",
    "5. hyperparameters_list: A list of dictionaries containing combinations of hyperparameters to try.\n",
    "* **Outputs:**\n",
    "1. best_model: The best LSTM model found.\n",
    "2. history: The training history of the best LSTM model.\n",
    "3. train_metric: The evaluation metric on the training set for the best LSTM model.\n",
    "4. best_mae: The evaluation metric on the validation set for the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "088c9758-a9a5-401e-97ce-248a9a095c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(X_train, y_train, X_test, y_test, hyperparameters_list):\n",
    "    \n",
    "    # Initialize variables to store the best hyperparameters and model\n",
    "    models = []\n",
    "    val_losses = []         # A list to store validation losses for each combination of hyperparameters\n",
    "    best_val_loss = np.inf  # The best validation loss observed so far\n",
    "    best_model = None       # The best model observed so far\n",
    "    best_activation = None  # The activation function that resulted in the best model\n",
    "    best_optimizer = None   # The optimizer that resulted in the best model\n",
    "    best_cv_method= None\n",
    "    \n",
    "    # Loop through all combinations of activations and optimizers\n",
    "    for hyperparams in hyperparameters_list:\n",
    "        n_units= hyperparams['lstm_units']\n",
    "        learning_rate= hyperparams['learning_rate']\n",
    "        epochs= hyperparams['epochs']\n",
    "        batch_size= hyperparams['batch_size']\n",
    "        dropout= hyperparams['dropout']\n",
    "        data_aug= hyperparams['data_augmentation']\n",
    "        activation = hyperparams['activation']\n",
    "        optimizer = hyperparams['optimizer']\n",
    "        n_units = hyperparams['lstm_units']\n",
    "        reg = hyperparams['regularization']\n",
    "        patience= hyperparams['early_stopping_patience']\n",
    "        pruning= hyperparams['pruning']\n",
    "        ens= hyperparms['ensembling']\n",
    "        \n",
    "        history= []\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\n",
    "        # Create the model with the given hyperparameters\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(n_units, activation=activation, input_shape=(input_steps, X_train.shape[2]), kernel_regularizer=keras.regularizers.l2(reg), return_sequences=True))\n",
    "        model.add(LSTM(n_units, activation=activation, input_shape=(input_steps, X_train.shape[2]), kernel_regularizer=keras.regularizers.l2(reg)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Add more LSTM layers\n",
    "        model.add(LSTM(n_units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg), return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(LSTM(n_units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Add a Dense output layer\n",
    "        model.add(Dense(output_steps))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer(lr=learning_rate), loss='mean_squared_error', metrics=metric)\n",
    "    \n",
    "        if data_aug and dropout >= 0:\n",
    "            datagen = TimeseriesGenerator(X_train, y_train, length=input_steps, batch_size=batch_size)\n",
    "            history = model.fit(datagen, epochs=epochs, verbose=1, callbacks=[es])\n",
    "            models.append(model)\n",
    "        elif ens:\n",
    "            models = []\n",
    "            for i in range(n_models):\n",
    "                model = Sequential()\n",
    "            \n",
    "                for i in range(random.randint(0, 20)):\n",
    "                    model.add(LSTM(n_units, activation=activation, input_shape=(input_steps, X_train.shape[2]), kernel_regularizer=keras.regularizers.l2(reg), return_sequences=True))\n",
    "                    model.add(Conv1D(filters=random.randint(21, 40), kernel_size=random.randint(41, 60), activation=activation, input_shape=(input_steps, X_train.shape[2])))\n",
    "                    model.add(LSTM(n_units, activation=activation, input_shape=(input_steps, X_train.shape[2]), kernel_regularizer=keras.regularizers.l2(reg)))\n",
    "                    model.add(Dropout(random.randint(0, 100)))\n",
    "                \n",
    "                # Add a Dense output layer\n",
    "                model.add(Dense(output_steps))\n",
    "\n",
    "                # Compile the model\n",
    "                model.compile(optimizer=optimizer(lr=learning_rate), loss='mean_squared_error', metrics=metric)\n",
    "\n",
    "                # Train the model\n",
    "                history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1)\n",
    "            \n",
    "                # Add the trained model to the list\n",
    "                models.append(model)\n",
    "                \n",
    "                # Use pruning to combine predictions if specified\n",
    "                if pruning:                  \n",
    "                    \n",
    "                    # Define hyperparameters and create the pruned LSTM model\n",
    "                    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "                    output_dim = 1\n",
    "                    dropout_rate = 0.2\n",
    "                    reg = 0.001\n",
    "                    threshold = 0.1\n",
    "                    model, pruner = create_pruned_lstm(input_shape, output_dim, dropout_rate, reg, threshold)\n",
    "\n",
    "                    # Train the model\n",
    "                    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1, callbacks=[pruner])\n",
    "\n",
    "                    # Make predictions on the test set\n",
    "                    y_pred = model.predict(X_test)  \n",
    "                          \n",
    "                    for model in models:\n",
    "                        y_pred = model.predict\n",
    "            \n",
    "            # Evaluate the performance of the ensemble model\n",
    "            mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "            print(f'Ensemble MAE: {mae_ensemble}')\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Train the model\n",
    "            history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1, callbacks=[es])\n",
    "\n",
    "        # Cross-validate the model using different methods\n",
    "        for cv_method_name, cv_method in cv_methods:\n",
    "            val_loss, model, history = cross_validate(X, y, hyperparams, cv_method_name, cv_method, input_steps, output_steps, metric)\n",
    "            print('Validation loss:', val_loss)\n",
    "            print('Hyperparameters:', hyperparams)              \n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "        # Update the best model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "            best_activation = activation\n",
    "            best_optimizer = optimizer.__name__\n",
    "            best_cv_method = cv_method\n",
    "            best_hyperparams = hyperparams\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print('Selected hyperparameters:', best_hyperparams)\n",
    "    print('Activation:', best_activation)\n",
    "    print('Optimizer:', best_optimizer)\n",
    "    print('Learning rate:', learning_rate)\n",
    "    print('Best validation loss:', best_val_loss)\n",
    "\n",
    "    # Return the best model, training history, and the corresponding evaluation metrics\n",
    "    y_pred_train = best_model.predict(X_train)\n",
    "    y_pred_val = best_model.predict(X_val)\n",
    "    train_metric = mae(y_train, y_pred_train)\n",
    "    best_mae = mae(y_val, y_pred_val)\n",
    "\n",
    "    return best_model, history, train_metric, best_mae, best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4a342",
   "metadata": {},
   "source": [
    "#### Machine Learning Applied\n",
    "\n",
    "The create_lstm_model function trains a Long Short-Term Memory (LSTM) model for time series prediction. LSTMs are a type of Recurrent Neural Network (RNN) that can learn and remember patterns over long sequences of data, making them particularly suitable for time series data. They overcome the limitations of traditional RNNs by addressing the vanishing and exploding gradient problems during training. This is achieved through the use of memory cells, input gates, output gates, and forget gates that regulate the flow of information and help the LSTM network maintain long-term dependencies.\n",
    "\n",
    "#### Mathematical Concepts and Statistical Analysis\n",
    "\n",
    "**Memory Cells:** Memory cells are the central component of LSTM units. They store the long-term information of the sequence. The memory cell state (denoted as C_t) is updated as follows:\n",
    "\n",
    "C_t = f_t * C_(t-1) + i_t * g_t\n",
    "\n",
    "Here, f_t is the forget gate output, i_t is the input gate output, and g_t is the candidate memory cell state.\n",
    "\n",
    "**Gates:** LSTM units have three gates: input, output, and forget gates. These gates control the flow of information into, out of, and within the LSTM unit. Gates perform element-wise multiplication using sigmoid activation functions, which output values between 0 and 1.\n",
    "\n",
    "_**1. Input Gate:**_ Controls the flow of new information into the memory cell. It is calculated as:\n",
    "\n",
    "i_t = sigmoid(W_i * [h_(t-1), x_t] + b_i)\n",
    "\n",
    "_**2. Forget Gate:**_ Controls the amount of past information to retain or forget. It is calculated as:\n",
    "\n",
    "f_t = sigmoid(W_f * [h_(t-1), x_t] + b_f)\n",
    "\n",
    "_**3. Output Gate:**_ Controls the output of the LSTM unit based on the current memory cell state. It is calculated as:\n",
    "\n",
    "o_t = sigmoid(W_o * [h_(t-1), x_t] + b_o)\n",
    "\n",
    "**Memory Cell State Update:** The memory cell state is updated using the input and forget gates, as well as the current input (x_t). The new memory cell state (C_t) is calculated using the equations mentioned above.\n",
    "\n",
    "**Hidden State Update:** The hidden state (h_t) represents the LSTM's output at time step t. It is calculated as follows:\n",
    "\n",
    "h_t = o_t * tanh(C_t)\n",
    "\n",
    "**Training the LSTM Model:** The LSTM model is trained using backpropagation through time (BPTT) and gradient descent optimization. The model learns the optimal weights and biases for the gates and memory cells to minimize a loss function, typically Mean Squared Error (MSE) for time series prediction tasks.\n",
    "\n",
    "**Regularization:** Regularization techniques like L2 regularization and dropout are employed to prevent overfitting. L2 regularization adds a penalty term to the loss function proportional to the squared magnitude of the weights, while dropout randomly sets a fraction of input units to 0 during training.\n",
    "\n",
    "**Hyperparameter Tuning:** This function tests multiple combinations of hyperparameters such as the number of LSTM units, learning rate, dropout rate, and more. It selects the best model by comparing validation losses. This process helps to find the most suitable architecture and training settings for the specific problem.\n",
    "\n",
    "By combining these mathematical concepts and statistical techniques, the create_lstm_model function trains anLSTM model that can capture complex patterns and long-term dependencies in time series data. The function iteratively trains models with different hyperparameters, evaluates their performance using cross-validation, and selects the best model based on the validation loss. This ensures that the chosen LSTM model has the best architecture and training settings for the specific problem, resulting in a more accurate and robust time series prediction.\n",
    "\n",
    "#### Cross Applications\n",
    "_The LSTM model trained by this function can be applied to various time series prediction tasks in different domains, such as:_\n",
    "\n",
    "1. Stock market prediction\n",
    "2. Weather forecasting\n",
    "3. Energy demand forecasting\n",
    "4. Sales forecasting\n",
    "5. Traffic flow prediction\n",
    "6. Human activity recognition\n",
    "\n",
    "Moreover, the concepts and techniques used in this function can be adapted for other types of neural networks or machine learning models. By making appropriate adjustments to the model architecture, hyperparameters, and optimization techniques, it's possible to create a similar function for training and selecting the best model for other tasks and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9767d",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "_The cross_validate function is designed to perform cross-validation on a dataset using a specified cross-validation method to evaluate a model's performance with given hyperparameters. Cross-validation is an essential step in the machine learning process, as it helps prevent overfitting and provides a more reliable estimate of a model's performance on unseen data. This function can be applied to various time series problems and can be adapted to work with different machine learning models and cross-validation techniques._\n",
    "\n",
    "#### Reasoning and Decisions Made\n",
    "1. The function initializes three lists to store the mean absolute errors, models, and training history for each fold of the cross-validation process.\n",
    "2. The function loops through the splits generated by the cross-validation method and prepares the input data for the LSTM model using the previously defined to_supervised function.\n",
    "3. For each fold, the model is trained and evaluated using the train_and_evaluate_model function, which calculates the mean absolute error, the model, and the training history.\n",
    "4. The function then calculates the average validation loss across all folds and returns the best model and training history based on the lowest validation loss.\n",
    "\n",
    "#### Local Outside References:\n",
    "to_supervised function \n",
    "\n",
    "#### Input\n",
    "* X: A 2D array of input features\n",
    "* y: A 1D array of target values\n",
    "* hyperparams: A dictionary containing hyperparameters for the model\n",
    "* cv_method_name: A string representing the name of the cross-validation method\n",
    "* cv_method: A cross-validation method object from scikit-learn\n",
    "* input_steps: The number of input time steps for the LSTM model\n",
    "* output_steps: The number of output time steps for the LSTM model\n",
    "* metric: A performance metric function from scikit-learn\n",
    "\n",
    "#### Outputs: \n",
    "* val_loss: The validation loss of the best preforming metric\n",
    "* best_model: The best preforming model\n",
    "* best_history: The history of ther best preforming model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "46a6a10f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validate(X, y, hyperparams, cv_method_name, cv_method, input_steps, output_steps, metric):\n",
    "    mae_list = []         # A list to store mean absolute errors for each fold\n",
    "    model_list = []       # A list to store models for each fold\n",
    "    history_list = []     # A list to store training history for each fold\n",
    "\n",
    "    try:\n",
    "        # Loop through the splits generated by the cross-validation method\n",
    "        for i, (train_idx, val_idx) in enumerate(cv_method.split(X)):\n",
    "            # Split the data into training and validation sets\n",
    "            X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "            y_train_cv, y_val_cv = np.array(y)[train_idx], np.array(y)[val_idx]\n",
    "\n",
    "            # Prepare input data for LSTM\n",
    "            X_train_cv, y_train_cv = to_supervised(X_train_cv, y_train_cv, input_steps, output_steps)\n",
    "            X_val_cv, y_val_cv = to_supervised(X_val_cv, y_val_cv, input_steps, output_steps)\n",
    "\n",
    "            # Train and evaluate the model\n",
    "            mae, model, history = train_and_evaluate_model(X_train_cv, y_train_cv, X_val_cv, y_val_cv, hyperparams, input_steps, output_steps, metric)\n",
    "            mae_list.append(mae)\n",
    "            model_list.append(model)\n",
    "            history_list.append(history)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{cv_method_name}: Is incompatible with the model. Removing from consideration. Error: {e}\")\n",
    "        val_loss = np.inf\n",
    "\n",
    "    # Calculate average validation loss across all folds\n",
    "    val_loss = np.mean(mae_list)\n",
    "\n",
    "    # Return average validation loss, best model, and training history\n",
    "    return val_loss, model_list[np.argmin(mae_list)], history_list[np.argmin(mae_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d2b25",
   "metadata": {},
   "source": [
    "#### Machine Learning Applied\n",
    "\n",
    "In the context of the cross_validate function, cross-validation is a widely-used technique in machine learning to evaluate a model's performance. It involves partitioning the dataset into multiple subsets or \"folds\" and iteratively training the model on a subset while testing it on the remaining data. This process helps ensure that the model's performance is not biased towards any specific part of the dataset and provides a more accurate estimate of how well the model generalizes to new, unseen data.\n",
    "\n",
    "#### Mathematical Concepts and Statistical Analysis:\n",
    "\n",
    "**Average Validation Loss:** After performing cross-validation, the function calculates the average validation loss across all folds by taking the mean of the mean absolute errors for each fold. This average provides a more reliable estimate of the model's performance on unseen data.\n",
    "\n",
    "#### Broader Picture and Cross Applications:\n",
    "\n",
    "The cross_validate function is a versatile tool that can be applied to various time series problems and machine learning models. By allowing users to specify different cross-validation methods, the function can be adapted to suit the unique characteristics and requirements of different datasets and problem domains.\n",
    "\n",
    "For instance, in the context of an organization, cross-validation can be used to evaluate the performance of machine learning models for forecasting sales, predicting equipment maintenance needs, or analyzing customer behavior patterns. By using cross-validation to optimize model performance, organizations can make more informed decisions, reduce costs, and improve overall efficiency.\n",
    "\n",
    "Furthermore, the cross_validate function can be extended to work with other machine learning models and cross-validation techniques. This flexibility allows for the exploration of various modeling approaches, ultimately leading to better performance and increased value creation within an organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b3fee",
   "metadata": {},
   "source": [
    "### Best Feature Selector\n",
    "_The feature_selection_function aims to identify the most important features within a dataset to improve the performance of a machine learning model. The function evaluates three different feature selection methods (Recursive Feature Elimination, Sequential Feature Selection, and Correlation-based feature selection) and selects the method that results in the lowest mean absolute error (MAE)._\n",
    "\n",
    "#### Local outside references:\n",
    "* select_features_rfe\n",
    "* select_features_sfs\n",
    "* feature_selection_correlation\n",
    "\n",
    "#### Inputs:\n",
    "* X_train: Training dataset features\n",
    "* y_train: Training dataset target variable\n",
    "* X_test: Test dataset features\n",
    "* y_test: Test dataset target variable\n",
    "\n",
    "#### Outputs:\n",
    "* X_train: Updated training dataset features after applying the best feature selection method\n",
    "* X_test: Updated test dataset features after applying the best feature selection method\n",
    "* feature_selection: The name of the best feature selection method\n",
    "* best_feature_selection_result: The tuple containing the best feature selection method name, the best MAE, and the selected features\n",
    "* The best feature selection result is printed as a side effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "7b5fe789-8175-4525-ad42-ac5d68b132b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function definition\n",
    "def feature_selection_function(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # Initialize variables to store best results\n",
    "    fbest_mae = float('inf')\n",
    "    fbest_method = None\n",
    "    ffeatures= None\n",
    "\n",
    "    # RFE (Recursive Feature Elimination)\n",
    "    X_rfe, mae_rfe = select_features_rfe(X_train, y_train, X_test, y_test)\n",
    "    if mae_rfe < best_mae:\n",
    "        # Update best results if RFE provides better performance\n",
    "        fbest_mae = mae_rfe\n",
    "        best_X = X_rfe\n",
    "        fbest_method = 'Recursive Feature Selection'\n",
    "        ffeatures= features\n",
    "        X_test= X_test\n",
    "\n",
    "    # SFS (Sequential Feature Selection)\n",
    "    X_sfs, mae_sfs = select_features_sfs(X_train, y_train, X_test, y_test)\n",
    "    if mae_sfs < best_mae:\n",
    "        # Update best results if SFS provides better performance\n",
    "        fbest_mae = mae_sfs\n",
    "        best_X = X_sfs\n",
    "        best_method = 'Sequential Feature Selection'\n",
    "        ffeatures= features\n",
    "        X_test= X_test\n",
    "        \n",
    "    # Correlation-based feature selection\n",
    "    X_corr, mae_corr = feature_selection_correlation(X_train, y_train, X_test, y_test)\n",
    "    if mae_corr < best_mae:\n",
    "        # Update best results if correlation-based selection provides better performance\n",
    "        fbest_mae = mae_corr\n",
    "        best_X = X_corr\n",
    "        fbest_method = 'Correlation'\n",
    "        ffeatures= features\n",
    "        X_test= X_test\n",
    "    \n",
    "    # Store the best feature selection results\n",
    "    best_feature_selection_results= (fbest_method, fbest_mae, features)\n",
    "    \n",
    "    # Return the updated training and test datasets, feature selection method, and results\n",
    "    return X_train, X_test, best_feature_selection_result,  print(best_feature_selection_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfbfde",
   "metadata": {},
   "source": [
    "Machine Learning Concepts:\n",
    "\n",
    "Recursive Feature Elimination (RFE): RFE is a technique that recursively removes features and builds a model on the remaining features. It uses the model accuracy to identify which features contribute the most to the prediction.\n",
    "\n",
    "Feature selection is an essential step in the machine learning pipeline as it helps in reducing the dimensionality of the dataset, which in turn reduces the complexity of the model, improves its performance, and reduces the risk of overfitting.\n",
    "Recursive Feature Elimination (RFE): RFE is a technique that recursively removes features and builds a model on the remaining features. It uses the model accuracy to identify which features contribute the most to the prediction.\n",
    "\n",
    "Sequential Feature Selection (SFS): SFS is a technique that iteratively adds or removes features based on their contribution to the model's performance. It can be implemented as forward, backward, or bidirectional selection.\n",
    "\n",
    "Correlation-based feature selection: This method selects features that have the highest correlation with the target variable while maintaining low correlation with each other.\n",
    "\n",
    "\n",
    "Mathematical Concepts and Statistical Analysis:\n",
    "\n",
    "Mean Absolute Error (MAE): The performance metric used in this function is the mean absolute error, calculated as the average of the absolute differences between the predicted and actual values.\n",
    "\n",
    "Correlation: Correlation is a measure of the strength and direction of the relationship between twovariables. It ranges from -1 to 1, where -1 indicates a strong negative relationship, 0 indicates no relationship, and 1 indicates a strong positive relationship.\n",
    "\n",
    "Broader Picture and Cross Applications:\n",
    "\n",
    "Feature selection techniques are widely applicable across various domains and industries. In the context of an organization, the feature_selection_function can be used to:\n",
    "\n",
    "Improve model performance: By selecting only the most relevant features, the model can make better predictions, which can lead to better decision-making and increased efficiency.\n",
    "\n",
    "Reduce computational resources: With fewer features, the model requires less memory and processing power, which can lead to cost savings, especially when dealing with large datasets.\n",
    "\n",
    "Enhance interpretability: A simpler model with fewer features is easier to understand and explain to stakeholders, facilitating better communication of the results.\n",
    "\n",
    "Identify important variables: Feature selection can help identify key drivers of a target variable, which can inform the development of new products, services, or strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07f42b",
   "metadata": {},
   "source": [
    "Explanation for Non-Coders:\n",
    "\n",
    "The select_features_rfe function uses Recursive Feature Elimination (RFE) for feature selection. It starts by initializing the number of features to select as 4 and the previous Mean Absolute Error (MAE) as infinity. It then iteratively trains an RFE model with a RandomForestRegressor estimator, fitting the model to the training data, and transforming the training and test datasets to include only the selected features. A Linear Regression model is then trained and evaluated on the selected features, calculating the MAE.\n",
    "\n",
    "The process continues until the MAE is within the range of -1 to 1 or if the new MAE is further from 0 than the previous one. In the latter case, the number of features to select is increased, and the previous MAE is updated. Finally, the function returns the transformed training and test datasets, the MAE, and the selected features.\n",
    "\n",
    "Broader Picture and Cross Applications:\n",
    "\n",
    "Recursive Feature Elimination (RFE) is a versatile and widely applicable feature selection technique. It can be used across various domains and industries to enhance model performance, reduce computational resources, improve interpretability, and identify important variables. It is particularly useful in situations where there are a large number of features and a need to identify a smaller subset that contributes the most to the target variable.\n",
    "\n",
    "Explanation of Machine Learning Concepts:\n",
    "\n",
    "RFE is a wrapper-based feature selection method that iteratively removes the least important features and fits a model to the remaining features. It uses an estimator, such as a RandomForestRegressor, to rank the importance of each feature. The least important features are eliminated, and the process continues until a specified number of features remain.\n",
    "Mathematical Concepts and Statistical Analysis:\n",
    "\n",
    "Recursive Feature Elimination (RFE) is built upon the concept of model-based feature importance. The RandomForestRegressor, which is used as the estimator in this example, measures feature importance by calculating the average impurity decrease across all decision trees in the forest. The impurity decrease is measured using Gini impurity or entropy.\n",
    "\n",
    "The Mean Absolute Error (MAE) is used as the evaluation metric in this function. MAE is calculated as the average of the absolute differences between the predicted and actual values:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "MAE = (1/n) * Î£|y_i - y'_i|\n",
    "where n is the number of samples, y_i is the actual value, and y'_i is the predicted value. MAE is a common metric for regression problems, as it provides an easily interpretable measure of the average prediction error.\n",
    "\n",
    "Reasoning and Decisions Made:\n",
    "\n",
    "The function begins with an initial number of features to select as 4, and it iteratively increases this number until the stopping condition is met. This process allows the function to search for the best number of features to retain, balancing model complexity and predictive performance.\n",
    "\n",
    "The stopping condition is based on the MAE. If the MAE is within the range of -1 to 1, the process stops, as this indicates a satisfactory level of prediction error. Alternatively, if the new MAE is further from 0 than the previous one, the process stops, as this indicates that adding more features is not improving the model's performance.\n",
    "\n",
    "Local Outside References and Things Needed to Be Imported:\n",
    "\n",
    "Imports:\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "Inputs and Outputs of the Code:\n",
    "\n",
    "Inputs:\n",
    "X_train: Training dataset features\n",
    "y_train: Training dataset target variable\n",
    "X_test: Test dataset features\n",
    "Outputs:\n",
    "X_train: Transformed training dataset with selected features\n",
    "X_test: Transformed test dataset with selected features\n",
    "mae: Mean Absolute Error of the model on the test dataset\n",
    "features: Selected features using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "59249e20",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1642003305.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\calve\\AppData\\Local\\Temp\\ipykernel_26332\\1642003305.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Explanation for Non-Coders:\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Explanation for Non-Coders:\n",
    "\n",
    "The select_features_rfe function uses Recursive Feature Elimination (RFE) for feature selection. It starts by initializing the number of features to select as 4 and the previous Mean Absolute Error (MAE) as infinity. It then iteratively trains an RFE model with a RandomForestRegressor estimator, fitting the model to the training data, and transforming the training and test datasets to include only the selected features. A Linear Regression model is then trained and evaluated on the selected features, calculating the MAE.\n",
    "\n",
    "The process continues until the MAE is within the range of -1 to 1 or if the new MAE is further from 0 than the previous one. In the latter case, the number of features to select is increased, and the previous MAE is updated. Finally, the function returns the transformed training and test datasets, the MAE, and the selected features.\n",
    "\n",
    "Broader Picture and Cross Applications:\n",
    "\n",
    "Recursive Feature Elimination (RFE) is a versatile and widely applicable feature selection technique. It can be used across various domains and industries to enhance model performance, reduce computational resources, improve interpretability, and identify important variables. It is particularly useful in situations where there are a large number of features and a need to identify a smaller subset that contributes the most to the target variable.\n",
    "\n",
    "Explanation of Machine Learning Concepts:\n",
    "\n",
    "RFE is a wrapper-based feature selection method that iteratively removes the least important features and fits a model to the remaining features. It uses an estimator, such as a RandomForestRegressor, to rank the importance of each feature. The least important features are eliminated, and the process continues until a specified number of features remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "a41efbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Selection Function Using Recursive Feature Elimination\n",
    "def select_features_rfe(X_train, y_train, X_test, y_test):\n",
    "    n_features_to_select = 4 # Initialize the number of features to select\n",
    "    prev_mae = float('inf') # Initialize the previous Mean Absolute Error (MAE) to infinity\n",
    "    \n",
    "    # Reshape X_train and y_train to 2D arrays\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], -1))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0], -1))\n",
    "    \n",
    "    while True: # Continue until a stopping condition is met\n",
    "        # Create RFE object with a RandomForestRegressor estimator and select 4 features\n",
    "        rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=4)\n",
    "        rfe.fit(X_train, y_train) \n",
    "        # Transform X_train and X_test to include only selected features\n",
    "        X_train = rfe.transform(X_train)\n",
    "        X_test = rfe.transform(X_test)\n",
    "        \n",
    "        # Train and evaluate the model\n",
    "        model = LinearRegression() # Create a Linear Regression model\n",
    "        model.fit(X_train, y_train) # Fit the model to the training data\n",
    "        y_pred = model.predict(X_test) # Generate predictions on the test data\n",
    "        mae = mean_absolute_error(y_test, y_pred) # Calculate the Mean Absolute Error (MAE)\n",
    "\n",
    "        # Check if the MAE is within the range of -1 to 1\n",
    "        if -1 <= mae <= 1:\n",
    "            break\n",
    "        # Check if the new MAE is further from 0 than the previous one\n",
    "        elif np.abs(mae) > np.abs(prev_mae):\n",
    "            break\n",
    "        else:\n",
    "            n_features_to_select += 1 # Increase the number of features to select\n",
    "            prev_mae = mae # Update the previous MAE to the current MAE\n",
    "            \n",
    "        # Get the boolean mask for the selected features\n",
    "        X_train_mask = rfe.get_support()\n",
    "\n",
    "        # Get the column names of the selected features\n",
    "        features = np.array(X.columns)[X_train_mask]\n",
    "\n",
    "    return X_train, X_test, mae, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "1c5b77fb-bf50-4ebc-bb55-a12ef8d5f408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Selection Using Sequential Feature Selection\n",
    "def select_features_sfs(X_train, y_train, X_test, y_test):\n",
    "    n_features_to_select = 4\n",
    "    prev_mae = float('inf')\n",
    "    \n",
    "    # Reshape X_train and y_train to 2D arrays\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], -1))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0], -1))\n",
    "\n",
    "    while True:\n",
    "        # Create SFS object with a RandomForestRegressor estimator and select n_features_to_select features\n",
    "        sfs = SequentialFeatureSelector(estimator=RandomForestRegressor(), n_features_to_select=n_features_to_select)\n",
    "        sfs.fit(X_train, y_train)\n",
    "\n",
    "        X_train = sfs.transform(X_train)\n",
    "        X_test = sfs.transform(X_test)\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "        # Check if the MAE is within the range of -1 to 1\n",
    "        if -1 <= mae <= 1:\n",
    "            break\n",
    "        # Check if the new MAE is further from 0 than the previous one\n",
    "        elif np.abs(mae) > np.abs(prev_mae):\n",
    "            break\n",
    "        else:\n",
    "            n_features_to_select += 1\n",
    "            prev_mae = mae\n",
    "                    \n",
    "    # Get the boolean mask for the selected features\n",
    "    X_train_mask = sfs.get_support()\n",
    "\n",
    "    # Get the column names of the selected features\n",
    "    features = np.array(X.columns)[X_train_mask]\n",
    "        \n",
    "    return X_train, X_test, mae, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "95f93298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_selection_correlation(X_train, y_train, X_test, y_test):\n",
    "    c_thresholds = [0.1, 0.3, 0.5]\n",
    "    best_threshold = 0\n",
    "    best_columns = None\n",
    "    \n",
    "    # Compute the correlation matrix while handling potential division by zero\n",
    "    stddev = np.std(X_train, axis=0, ddof=1)\n",
    "    X_train_norm = (X_train - np.mean(X_train, axis=0)) / (stddev.reshape(1, -1) + 1e-8)\n",
    "    corr_matrix = np.dot(X_train_norm.T, X_train_norm) / (X_train_norm.shape[0] - 1)\n",
    "\n",
    "    # Loop through each threshold and select the features with correlation above that threshold\n",
    "    for c_threshold in c_thresholds:\n",
    "        columns = np.full((corr_matrix.shape[0],), True, dtype=bool)\n",
    "        for i in range(corr_matrix.shape[0]):\n",
    "            for j in range(i+1, corr_matrix.shape[0]):\n",
    "                if corr_matrix[i, j] >= c_threshold:\n",
    "                    if columns[j]:\n",
    "                        columns[j] = False\n",
    "\n",
    "        # Check if the current threshold results in a better set of columns than the previous best\n",
    "        if best_columns is None or sum(columns) > sum(best_columns):\n",
    "            best_threshold = c_threshold\n",
    "            best_columns = columns\n",
    "\n",
    "    # Use the best set of columns found\n",
    "    X_train = X_train[:, best_columns]\n",
    "    X_test = X_test[:, best_columns]\n",
    "    features = column_names[best_columns]\n",
    "\n",
    "    return X_train, X_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "aa8c7507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tensorflow import keras\n",
    "def generate_hyperparameter_list():\n",
    "    # Dictionary of hyperparameter options, with keys as the names of the hyperparameters\n",
    "    # and values as arrays of possible values\n",
    "    hyperparameters_options = {\n",
    "        'lstm_units': [32, 64, 128],\n",
    "        'activation': ['relu', 'sigmoid', 'tanh'],\n",
    "        'dropout': [0, 0.1, 0.2, 0.3],\n",
    "        'regularization': [0.001, 0.01, 0.1],\n",
    "        'optimizer': [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'batch_size': [32, 64],\n",
    "        'epochs': [50, 100],\n",
    "        'early_stopping_patience': [5, 10, 15],\n",
    "        'threshold': [0.1, 0.5, 0.9],\n",
    "        'data_augmentation': [False, True],\n",
    "        'ensembling': [False, True],\n",
    "        'pruning': [False, True],\n",
    "    }\n",
    "    # Unzip the dictionary to get separate lists of hyperparameter names and possible values\n",
    "    keys, values = zip(*hyperparameters_options.items())\n",
    "    # Get all possible combinations of hyperparameter values and create a list of dictionaries\n",
    "    hyperparameter_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    return hyperparameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "104a9a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#4\n",
    "class HoldoutMethod:\n",
    "    def __init__(self, test_size=0.2):\n",
    "        self.test_size = test_size\n",
    "    \n",
    "    # Method for splitting the data into train and test sets using the Hold Out method\n",
    "    def split(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_test = int(self.test_size * n_samples)\n",
    "        test_indices = np.random.choice(n_samples, n_test, replace=False)\n",
    "        train_indices = np.setdiff1d(np.arange(n_samples), test_indices)\n",
    "        # Returns a list containing tuples of train and test indices\n",
    "        return [(train_indices, test_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "5d7854fb-056b-493c-9382-22d21898afdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pruner:\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, epoch, logs=None):\n",
    "        # Define the pruning callback\n",
    "        pruning_params = {'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                                       final_sparsity=self.threshold,\n",
    "                                                                       begin_step=2000,\n",
    "                                                                       end_step=4000)}\n",
    "        callbacks = [sparsity.UpdatePruningStep(), sparsity.PruningSummaries(log_dir='./logs', profile_batch=0)]\n",
    "        callbacks.append(sparsity.PruningCallback(pruning_params, verbose=1))\n",
    "\n",
    "        return callbacks\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
